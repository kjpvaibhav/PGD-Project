# -*- coding: utf-8 -*-
"""13-02-25.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/XXXXXXXXXX
    https://github.com/kjpvaibhav/PGD-Project

**Name:** Kamarajugadda Jyothi Phani Vaibhav

**PNR:** 240860825003

**Problem Statement:**

In the field of healthcare, particularly in ophthalmology, early diagnosis of eye diseases is crucial for effective treatment and prevention of blindness. This project focuses on developing a deep learning model that can automatically classify different types of ocular diseases from fundus images (photographs of the back of the eye). The goal is to create a predictive model that can detect multiple eye conditions, such as diabetes, glaucoma, cataracts, hypertension, age-related macular degeneration (AMD), and more, based on features extracted from these images. Given a dataset of ocular images and diagnostic keywords like ODIR-5K, the model, like Swin Transformer, should accurately classify and predict whether a given eye image is associated with any of these diseases, helping doctors and healthcare professionals with faster and more accurate diagnoses.



**Objective:**

To develop a Deep Learning model to classify the Ocular disease type, using the Microsoft Swin Transformer on the ODIR-5K dataset

# Importing Required Modules
"""

# For Dataset Download
import kagglehub

# Data Processing
import numpy as np
import pandas as pd

# Data Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# NTLK
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from wordcloud import WordCloud, STOPWORDS
from nltk.stem import SnowballStemmer

# Machine Learning
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, cohen_kappa_score, confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score, roc_curve
from sklearn.preprocessing import LabelEncoder
from sklearn.utils.class_weight import compute_class_weight

# Deep Learning
import timm
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from transformers import SwinForImageClassification

# Miscellaneous
import os
import itertools
import string
import re
from random import sample
from PIL import Image
import warnings
warnings.filterwarnings("ignore")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print(f"Using device: {device}")

"""# Loading Dataset"""

path = kagglehub.dataset_download("andrewmvd/ocular-disease-recognition-odir5k")

print("Path to dataset files:", path)

"""# Exploratory Data Analysis"""

import os

def get_folder_info(parent_dir):
    folder_info = []

    for dirpath, dirnames, filenames in os.walk(parent_dir):
        total_size = 0
        num_folders = len(dirnames)
        num_files = len(filenames)

        for filename in filenames:
            filepath = os.path.join(dirpath, filename)
            total_size += os.path.getsize(filepath)

        if num_files == 1:
            single_file_size = os.path.getsize(os.path.join(dirpath, filenames[0]))
            folder_info.append({
                'folder_name': dirpath,
                'num_folders': num_folders,
                'num_files': num_files,
                'total_size': single_file_size,
                'filenames': filenames[0]
            })
        else:
            folder_info.append({
                'folder_name': dirpath,
                'num_folders': num_folders,
                'num_files': num_files,
                'total_size': total_size,
                'filenames': "Image Files"
            })

    return folder_info

folders = get_folder_info(path)

for folder in folders:
    print(f"Folder: {folder['folder_name']}")
    print(f"Number of subfolders: {folder['num_folders']}")
    print(f"Number of files: {folder['num_files']}")
    print(f"Total size: {folder['total_size']} bytes")
    print('=' * 40)

    if folder['num_files'] == 1:
        single_file = folder['filenames']
        single_file_path = os.path.join(folder['folder_name'], single_file)
        print(f"Single file: {single_file}")
        print(f"Total size: {folder['total_size']} bytes")
        print(f"File path: {single_file_path}")
        print('-' * 40)

eyes = pd.read_csv(path + '/full_df.csv')
eyes

eyes.info()

eyes.describe(include="all")

eyes.columns

len(eyes)

len(eyes[eyes.duplicated()])

eyes.isnull().sum()

label_columns = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']
label_names = [
    "Normal",
    "Diabetic Retinopathy",
    "Glaucoma",
    "Cataract",
    "Age-related Macular Degeneration",
    "Hypertension",
    "Myopia",
    "Other diseases"
]

labels_dic = dict(zip(label_columns, label_names))
print(labels_dic)

print("Count of rows containing 'right':", eyes['filename'].str.contains('right').sum())
print("Count of rows containing 'left':", eyes['filename'].str.contains('left').sum())

"""# Data Cleaning"""

eyes['Left-Fundus'].nunique()

photo_counts = eyes['Left-Fundus'].value_counts()
photo_more_than_once = photo_counts[photo_counts > 1].index.tolist()
print(len(photo_more_than_once))

photo_more_than_once[0]

eyes[eyes['Left-Fundus'] == '0_left.jpg']

print(len(eyes[(eyes['labels'] == "['N']") & (eyes['N'] != 1)]))
print(len(eyes[(eyes['labels'] == "['D']") & (eyes['D'] != 1)]))
print(len(eyes[(eyes['labels'] == "['O']") & (eyes['O'] != 1)]))
print(len(eyes[(eyes['labels'] == "['C']") & (eyes['C'] != 1)]))
print(len(eyes[(eyes['labels'] == "['G']") & (eyes['G'] != 1)]))
print(len(eyes[(eyes['labels'] == "['A']") & (eyes['A'] != 1)]))
print(len(eyes[(eyes['labels'] == "['M']") & (eyes['M'] != 1)]))
print(len(eyes[(eyes['labels'] == "['H']") & (eyes['H'] != 1)]))

eyes.drop(columns=[ 'ID'] , inplace=True)

old_path_part = "../input/ocular-disease-recognition-odir5k/ODIR-5K/Training Images/"
new_path_part = path + "/ODIR-5K/ODIR-5K/Training Images/"
eyes['filepath'] = eyes['filepath'].replace(old_path_part, new_path_part, regex=True)

for filepath in eyes['filepath']:
    assert os.path.exists(filepath), f"Missing file: {filepath}"

len(eyes[eyes['Patient Age'] == 1])

eyes['labels'] = eyes['labels'].str[2]
eyes.head()

eyes['labels'].nunique()

eyes['labels'].value_counts()

"""# Data Visualization"""

plt.figure(figsize=(14, 5))
sns.countplot(x='labels', data=eyes , orient='h')
plt.title('Count of Target Labels in Eyes Dataset')
plt.xlabel('Labels')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()

eyes[eyes['Left-Diagnostic Keywords'] == 'low image quality']

eyes = eyes.loc[~(eyes['Left-Diagnostic Keywords'] == 'low image quality')]

print(len(eyes[(eyes['labels'] == "['N']") & (eyes['N'] != 1)]))
print(len(eyes[(eyes['labels'] == "['O']") & (eyes['O'] != 1)]))
eyes.head()

photos_unique = eyes.drop_duplicates(subset='Left-Fundus', keep='first')
eyes = photos_unique
eyes.reset_index(drop=True,inplace=True)
len(eyes)

eyes['Left-Diagnostic Keywords'].nunique()

eyes['Right-Diagnostic Keywords'].nunique()

eyes['Left-Diagnostic Keywords'].mode()

eyes['Left-Diagnostic Keywords'].value_counts()

eyes['Right-Diagnostic Keywords'].mode()

both_eyes_normal = eyes[
    (eyes['Right-Diagnostic Keywords'] == 'normal fundus') &
    (eyes['Left-Diagnostic Keywords'] == 'normal fundus')
]

both_eyes_normal.reset_index(inplace=True,drop=True)

len(both_eyes_normal)

both_eyes_not_normal = eyes[
    (eyes['Right-Diagnostic Keywords'] != 'normal fundus') &
    (eyes['Left-Diagnostic Keywords'] != 'normal fundus')
]

both_eyes_not_normal.reset_index(inplace=True,drop=True)

len(both_eyes_not_normal)

right_eye_normal = eyes[
    (eyes['Right-Diagnostic Keywords'] == 'normal fundus') &
    (eyes['Left-Diagnostic Keywords'] != 'normal fundus')
]

right_eye_normal.reset_index(inplace=True,drop=True)

len(right_eye_normal)

left_eye_normal = eyes[
    (eyes['Right-Diagnostic Keywords'] != 'normal fundus') &
    (eyes['Left-Diagnostic Keywords'] == 'normal fundus')
]

left_eye_normal.reset_index(inplace=True,drop=True)

len(left_eye_normal)

sns.pairplot(eyes)
plt.show()

data_num = eyes[['Patient Age', 'N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']]
data_num.head()

corr_exists = data_num.corr()
corr_exists

plt.figure(figsize=(15, 15))
sns.heatmap(corr_exists, annot=True)
plt.title('Correlation Heatmap of ODIR Dataset')
plt.show()

fig, axes = plt.subplots(2, 2, figsize=(14, 10))
fig.suptitle('Disease Distribution Across Different Groups', fontsize=16)

titles = [
    'Both Eyes Normal',
    'Both Eyes Not Normal',
    'Right Eye Normal',
    'Left Eye Normal'
]

dataframes = [both_eyes_normal, both_eyes_not_normal, right_eye_normal, left_eye_normal]

for df, ax, title in zip(dataframes, axes.ravel(), titles):
    disease_columns = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']
    disease_counts = df[disease_columns].sum()

    sns.barplot(x=disease_counts.index, y=disease_counts.values, color='skyblue', ax=ax)
    ax.set_title(title)
    ax.set_xlabel('Diseases')
    ax.set_ylabel('Frequency')

plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()

fig, axes = plt.subplots(2, 2, figsize=(14, 10))
fig.suptitle('Disease Distribution Across Different Groups', fontsize=16)

titles = [
    'Both Eyes Normal',
    'Both Eyes Not Normal',
    'Right Eye Normal',
    'Left Eye Normal'
]

dataframes = [both_eyes_normal, both_eyes_not_normal, right_eye_normal, left_eye_normal]

for df, ax, title in zip(dataframes, axes.ravel(), titles):
    disease_columns = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']
    disease_counts = df[disease_columns].sum()

    sns.barplot(x=disease_counts.index, y=disease_counts.values, color='skyblue', ax=ax)
    ax.set_title(title)
    ax.set_xlabel('Diseases')
    ax.set_ylabel('Frequency')

plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()

condition_normal_left = eyes['Left-Diagnostic Keywords'] == 'normal fundus'
condition_normal_right = eyes['Right-Diagnostic Keywords'] == 'normal fundus'

both_normal = (condition_normal_left) & (condition_normal_right)
both_abnormal = (~condition_normal_left) & (~condition_normal_right)
left_normal_right_abnormal = (condition_normal_left) & (~condition_normal_right)
right_normal_left_abnormal = (~condition_normal_left) & (condition_normal_right)

counts = {
    'Both Normal': both_normal.sum(),
    'Both Abnormal': both_abnormal.sum(),
    'Left Normal, Right Abnormal': left_normal_right_abnormal.sum(),
    'Right Normal, Left Abnormal': right_normal_left_abnormal.sum()
}

labels = counts.keys()
sizes = counts.values()
colors = ['#ff9999','#66b3ff','#99ff99','#ffcc99']
explode = (0.1, 0.07, 0, 0)

plt.figure(figsize=(8, 6))
plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)
plt.title('Distribution of Fundus Conditions')
plt.axis('equal')

plt.show()

fig, axes = plt.subplots(2, 2, figsize=(8, 8), sharex=True, sharey=True)

dataframes = [both_eyes_normal, both_eyes_not_normal, right_eye_normal, left_eye_normal]
titles = ['Both Eyes Normal', 'Both Eyes Not Normal', 'Right Eye Normal', 'Left Eye Normal']

for i, (df, title) in enumerate(zip(dataframes, titles)):
    row = i // 2
    col = i % 2
    sns.histplot(df['Patient Age'], kde=True, bins=10, ax=axes[row, col])
    axes[row, col].set_title(title)
    axes[row, col].set_xlabel('Age')
    axes[row, col].set_ylabel('Frequency')


plt.tight_layout()
plt.show()

counts = {
    'Both Eyes Normal': both_eyes_normal['Patient Sex'].value_counts(),
    'Both Eyes Not Normal': both_eyes_not_normal['Patient Sex'].value_counts(),
    'Right Eye Normal': right_eye_normal['Patient Sex'].value_counts(),
    'Left Eye Normal': left_eye_normal['Patient Sex'].value_counts()
}

plot_data = pd.DataFrame(counts).fillna(0).T.reset_index()
plot_data = plot_data.melt(id_vars='index', var_name='Gender', value_name='Count')
plot_data = plot_data.rename(columns={'index': 'Category'})

plt.figure(figsize=(8, 8))
sns.barplot(data=plot_data, x='Category', y='Count', hue='Gender', palette='viridis')

plt.title('Number of Males and Females by Category')
plt.xlabel('Category')
plt.ylabel('Number of Patients')
plt.legend(title='Gender')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

fig, axes = plt.subplots(2, 2, figsize=(8, 8), sharex=True, sharey=True)

dataframes = [both_eyes_normal, both_eyes_not_normal, right_eye_normal, left_eye_normal]
titles = ['Both Eyes Normal', 'Both Eyes Not Normal', 'Right Eye Normal', 'Left Eye Normal']
colors = ['#66b3ff', '#ff9999']

for i, (df, title) in enumerate(zip(dataframes, titles)):
    row = i // 2
    col = i % 2
    sns.histplot(df, x='Patient Age', hue='Patient Sex', multiple='stack', palette=colors, bins=10, ax=axes[row, col], kde=True)
    axes[row, col].set_title(title)
    axes[row, col].set_xlabel('Age')
    axes[row, col].set_ylabel('Frequency')

plt.tight_layout()
plt.show()

fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharex=True, sharey=True)

dataframes = [both_eyes_not_normal, right_eye_normal, left_eye_normal]

titles = ['Both Eyes Not Normal', 'Right Eye Normal', 'Left Eye Normal']
colors = ['#66b3ff', '#ff9999']

for i, (df, title) in enumerate(zip(dataframes, titles)):
    row = i // 3
    col = i % 3
    filtered_df = df.query('D == 1')
    if not filtered_df.empty:
        sns.histplot(filtered_df, x='Patient Age', hue='Patient Sex', multiple='stack', palette=colors, bins=10, ax=axes[col], kde=True)
    axes[col].set_title(title)
    axes[col].set_xlabel('Age')
    axes[col].set_ylabel('Frequency')

plt.tight_layout()
plt.show()

fig, ax = plt.subplots(figsize=(6, 6))

df = both_eyes_not_normal
title = 'Both Eyes Not Normal (H=1)'
colors = ['#66b3ff', '#ff9999']

filtered_df = df.query('H == 1')
if not filtered_df.empty:
    sns.histplot(filtered_df, x='Patient Age', hue='Patient Sex', multiple='stack', palette=colors, bins=10, ax=ax, kde=True)
    ax.set_title(title)
    ax.set_xlabel('Age')
    ax.set_ylabel('Frequency')
else:
    ax.text(0.5, 0.5, 'No data available for H = 1', horizontalalignment='center', verticalalignment='center', fontsize=14, color='red')

plt.tight_layout()
plt.show()

filtered_eyes = eyes[eyes['labels'] == 'G']
sample_images = sample(filtered_eyes['filepath'].tolist(), 16)

plt.figure(figsize=(12, 12))

for i in range(16):
    image = Image.open(sample_images[i])

    label = filtered_eyes.iloc[i]['labels']

    plt.subplot(4, 4, i+1)
    plt.imshow(image)
    plt.title(label, color='k', fontsize=12)
    plt.axis("off")

plt.show()

stemmer = SnowballStemmer("english")
stopword = set(stopwords.words('english'))

def clean(text):
    text = str(text).lower()

    text = re.sub(r'\[.*?\]', '', text)
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    text = re.sub(r'<.*?>+', '', text)
    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub(r'\n', '', text)
    text = re.sub(r'\w*\d\w*', '', text)

    text = [word for word in text.split() if word not in stopword]
    text = " ".join(text)

    text = [stemmer.stem(word) for word in text.split()]
    text = " ".join(text)

    return text

eyes['Right-Diagnostic Keywords'] = eyes['Right-Diagnostic Keywords'].apply(clean)

text = " ".join(i for i in both_eyes_not_normal['Right-Diagnostic Keywords'])
stopwords = set(STOPWORDS)
wordcloud = WordCloud(stopwords=stopwords, background_color="white").generate(text)
plt.figure(figsize=(15, 10))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

text = " ".join(i for i in both_eyes_not_normal['Left-Diagnostic Keywords'])
stopwords = set(STOPWORDS)
wordcloud = WordCloud(stopwords=stopwords, background_color="white").generate(text)
plt.figure(figsize=(15, 10))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

"""# Model Building"""

eyes = eyes[['filename', 'filepath', 'labels']]

label_encoder = LabelEncoder()

eyes['labels_encoded'] = label_encoder.fit_transform(eyes['labels'])

for label, encoded_value in zip(label_encoder.classes_, range(len(label_encoder.classes_))):
    print(f"{label}: {encoded_value}")

eyes.head()

train_df, test_df = train_test_split(eyes, test_size=0.2, random_state=42, stratify=eyes['labels_encoded'])
train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42, stratify=train_df['labels_encoded'])

class ODIRDataset(Dataset):
    def __init__(self, dataframe, transform=None):
        self.dataframe = dataframe
        self.transform = transform

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        img_path = self.dataframe.iloc[idx]['filepath']
        image = Image.open(img_path).convert('RGB')
        label = self.dataframe.iloc[idx]['labels_encoded']

        if self.transform:
            image = self.transform(image)

        return image, label

train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(20),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

val_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

train_dataset = ODIRDataset(train_df, transform=train_transform)
val_dataset = ODIRDataset(val_df, transform=val_transform)
test_dataset = ODIRDataset(test_df, transform=val_transform)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

"""# Model Development"""

model = timm.create_model('swin_tiny_patch4_window7_224', pretrained=True, num_classes=8)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

class_weights = compute_class_weight('balanced', classes=np.unique(eyes['labels_encoded']), y=eyes['labels_encoded'])
class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)

criterion = nn.CrossEntropyLoss(weight=class_weights)
optimizer = optim.AdamW(model.parameters(), lr=1e-4)
scheduler = StepLR(optimizer, step_size=5, gamma=0.1)

save_dir = '/content/downloadable'
os.makedirs(save_dir, exist_ok=True)

if os.path.exists(os.path.join(save_dir, 'training_history.csv')):
  training_history_df = pd.read_csv(os.path.join(save_dir, 'training_history.csv'))
else:
  training_history_df = pd.DataFrame(columns=["Epoch", "Train Loss", "Val Loss", "Val Acc"])

def train(model, dataloader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    for images, labels in dataloader:
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    return running_loss / len(dataloader)

def validate(model, dataloader, criterion, device):
    model.eval()
    val_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            val_loss += loss.item()

            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    return (val_loss / len(dataloader)), (correct / total)

def training_epoch(training_history_df, prev=0):
  NUM_OF_EPOCHS = 10+prev
  for epoch in range(prev, NUM_OF_EPOCHS):
      train_loss = train(model, train_loader, criterion, optimizer, device)
      val_loss, val_acc = validate(model, val_loader, criterion, device)
      scheduler.step()
      print(f"Epoch {epoch+1}/{NUM_OF_EPOCHS}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc: .4f}")

      new_row = pd.DataFrame([{"Epoch": epoch+1, "Train Loss": train_loss, "Val Loss": val_loss, "Val Acc": val_acc}])
      if not new_row.empty:
        training_history_df = pd.concat([training_history_df, new_row], ignore_index=True)
        training_history_df.to_csv(os.path.join(save_dir, 'training_history.csv'), index=False)
      epoch_model_path = os.path.join(save_dir, f'swin_model_epoch_{epoch+1}.pth')
      torch.save(model.state_dict(), epoch_model_path)
  torch.save(model.state_dict(), os.path.join(save_dir, 'swin_model_final.pth'))


if os.path.exists(os.path.join(save_dir, 'swin_model_final.pth')):
  print("Swin Model Found!\nLoading the model!")
  ans = input("Is it your final model (Y) or Do you want to train further (N):")
  if ans.lower() == 'n':
    model.load_state_dict(torch.load(os.path.join(save_dir, 'swin_model_final.pth')))
    prev = len(training_history_df)
    training_epoch(training_history_df, prev)
else:
    training_epoch(training_history_df)

"""# Model Evaluation"""

model.eval()

all_preds = []
all_labels = []
all_probs = []

with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        probs = torch.softmax(outputs, dim=1) 
        _, preds = torch.max(outputs, 1)

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())
        all_probs.extend(probs.cpu().numpy())

all_preds = np.array(all_preds)
all_labels = np.array(all_labels)
all_probs = np.array(all_probs)

accuracy = accuracy_score(all_labels, all_preds)
print(f'Accuracy: {accuracy:.4f}')

print('\nClassification Report:')
print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))

kappa = cohen_kappa_score(all_labels, all_preds)
print(f'Cohen Kappa Score: {kappa:.4f}')

f1 = f1_score(all_labels, all_preds, average='weighted')
print(f'F1 Score (Weighted): {f1:.4f}')

precision = precision_score(all_labels, all_preds, average='weighted')
print(f'Precision (Weighted): {precision:.4f}')

recall = recall_score(all_labels, all_preds, average='weighted')
print(f'Recall (Weighted): {recall:.4f}')

roc_auc = roc_auc_score(all_labels, all_probs, multi_class='ovr', average='weighted')
print(f'ROC AUC Score (Weighted): {roc_auc:.4f}')

n_classes = len(label_encoder.classes_)
fpr = dict()
tpr = dict()
roc_auc = dict()

for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(all_labels == i, all_probs[:, i])
    roc_auc[i] = roc_auc_score(all_labels == i, all_probs[:, i])

plt.figure(figsize=(10, 8))
for i in range(n_classes):
    plt.plot(fpr[i], tpr[i], label=f'Class {label_encoder.classes_[i]} (AUC = {roc_auc[i]:.2f})')

plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Each Class')
plt.legend(loc='lower right')
plt.show()

conf_matrix = confusion_matrix(all_labels, all_preds)
print('\nConfusion Matrix:')
print(conf_matrix)

plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=label_encoder.classes_,
            yticklabels=label_encoder.classes_)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

def sensitivity_specificity_multiclass(conf_matrix):
    num_classes = conf_matrix.shape[0]
    sensitivity = []
    specificity = []

    for i in range(num_classes):
        TP = conf_matrix[i, i]
        FN = sum(conf_matrix[i, :]) - TP
        FP = sum(conf_matrix[:, i]) - TP
        TN = conf_matrix.sum() - (TP + FP + FN)

        sens = TP / (TP + FN) if (TP + FN) != 0 else 0
        spec = TN / (TN + FP) if (TN + FP) != 0 else 0

        sensitivity.append(sens)
        specificity.append(spec)

    return sensitivity, specificity

sensitivity, specificity = sensitivity_specificity_multiclass(conf_matrix)

print('\nSensitivity and Specificity for Each Class:')
for i, (sens, spec) in enumerate(zip(sensitivity, specificity)):
    print(f'Class {label_encoder.classes_[i]}:')
    print(f'  Sensitivity: {sens:.4f}')
    print(f'  Specificity: {spec:.4f}')

def accuracy_per_class(conf_matrix):
    num_classes = conf_matrix.shape[0]
    class_accuracy = []

    for i in range(num_classes):
        TP = conf_matrix[i, i]
        FN = sum(conf_matrix[i, :]) - TP
        FP = sum(conf_matrix[:, i]) - TP
        TN = conf_matrix.sum() - (TP + FP + FN)

        acc = (TP + TN) / conf_matrix.sum() if conf_matrix.sum() != 0 else 0
        class_accuracy.append(acc)

    return class_accuracy

class_accuracy = accuracy_per_class(conf_matrix)

print('\nAccuracy for Each Class:')
for i, acc in enumerate(class_accuracy):
    print(f'Class {label_encoder.classes_[i]}: {acc:.4f}')

plt.figure(figsize=(10,6))

plt.plot(training_history_df['Epoch'], training_history_df['Train Loss'], label='Train Loss', color='blue')
plt.plot(training_history_df['Epoch'], training_history_df['Val Loss'], label='Validation Loss', color='red')

plt.title('Training and Validation Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()